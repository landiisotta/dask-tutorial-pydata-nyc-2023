{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask DataFrame - parallelized pandas\n",
    "\n",
    "Looks and feels like the pandas API, but for parallel and distributed workflows. \n",
    "\n",
    "At its core, the `dask.dataframe` module implements a \"blocked parallel\" `DataFrame` object that looks and feels like the pandas API, but for parallel and distributed workflows. One Dask `DataFrame` is comprised of many in-memory pandas `DataFrame`s separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-dataframe.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask DataFrame is composed of pandas DataFrames\"/>\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "## When to use `dask.dataframe`\n",
    "\n",
    "pandas is great for tabular datasets that fit in memory. A general rule of thumb for pandas is:\n",
    "\n",
    "> \"Have 5 to 10 times as much RAM as the size of your dataset\"\n",
    ">\n",
    "> ~ Wes McKinney (2017) in [10 things I hate about pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)\n",
    "\n",
    "Here \"size of dataset\" means dataset size on _the disk_.\n",
    "\n",
    "Dask becomes useful when the datasets exceed the above rule.\n",
    "\n",
    "In this notebook, you will be working with the New York City Airline data. This dataset is only ~200MB, so that you can download it in a reasonable time, but `dask.dataframe` will scale to  datasets **much** larger than memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a distributed Cluster in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the datasets you will be using in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99202bbb87474cfebe889fd5c1c07645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03941d32fa7455b86a990305c5aa598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    account=\"dask-tutorials\",\n",
    "    n_workers=20,\n",
    "    shutdown_on_close=False,\n",
    "    region=\"us-east-2\",\n",
    ")\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Diagnostic Dashboard\n",
    "\n",
    "Dask Distributed provides a useful Dashboard to visualize the state of your cluster and computations.\n",
    "\n",
    "If you're on **JupyterLab**, you can use the [Dask JupyterLab extension](https://github.com/dask/dask-labextension) (which should be already installed in your environment) to open the dashboard plots:\n",
    "* Click on the Dask logo in the left sidebar\n",
    "* Click on the magnifying glass icon, which will automatically connect to the active dashboard (if that doesn't work, you can type/paste the dashboard link http://127.0.0.1:8787 in the field)\n",
    "* Click on **\"Task Stream\"**, **\"Progress Bar\"**, and **\"Worker Memory\"**, which will open these plots in new tabs\n",
    "* Re-organize the tabs to suit your workflow!\n",
    "\n",
    "Alternatively, click on the dashboard link displayed in the Client details above: http://127.0.0.1:8787/status. It will open a new browser tab with the Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NYC Uber/Lyft rides\n",
    "\n",
    "The NYC Taxi dataset is a timeless classic.  \n",
    "\n",
    "Interestingly there is a new variant.  The NYC Taxi and Livery Commission requires data from all ride-share services in the city of New York.  This includes private limosine services, van services, and a new category \"High Volume For Hire Vehicle\" services, those that dispatch 10,000 rides per day or more.  This is a special category defined for Uber and Lyft.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we import the module `dask.dataframe` as `dd`, and call the corresponding `DataFrame` object `ddf`.\n",
    "\n",
    "**Note**: The term \"Dask DataFrame\" is slightly overloaded. Depending on the context, it can refer to the module or the DataFrame object. To avoid confusion, throughout this notebook:\n",
    "- `dask.dataframe` (note the all lowercase) refers to the API, and\n",
    "- `DataFrame` (note the CamelCase) refers to the object.\n",
    "\n",
    "The following dataset takes up around 100GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=720</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>datetime64[us]</td>\n",
       "      <td>datetime64[us]</td>\n",
       "      <td>datetime64[us]</td>\n",
       "      <td>datetime64[us]</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                hvfhs_license_num dispatching_base_num originating_base_num request_datetime on_scene_datetime pickup_datetime dropoff_datetime PULocationID DOLocationID trip_miles trip_time base_passenger_fare    tolls      bcf sales_tax congestion_surcharge airport_fee     tips driver_pay shared_request_flag  shared_match_flag access_a_ride_flag   wav_request_flag     wav_match_flag\n",
       "npartitions=720                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "                           string               string               string   datetime64[us]    datetime64[us]  datetime64[us]   datetime64[us]        int32        int32    float32     int32             float32  float32  float32   float32              float32     float32  float32    float32   category[unknown]  category[unknown]  category[unknown]  category[unknown]  category[unknown]\n",
       "                              ...                  ...                  ...              ...               ...             ...              ...          ...          ...        ...       ...                 ...      ...      ...       ...                  ...         ...      ...        ...                 ...                ...                ...                ...                ...\n",
       "...                           ...                  ...                  ...              ...               ...             ...              ...          ...          ...        ...       ...                 ...      ...      ...       ...                  ...         ...      ...        ...                 ...                ...                ...                ...                ...\n",
       "                              ...                  ...                  ...              ...               ...             ...              ...          ...          ...        ...       ...                 ...      ...      ...       ...                  ...         ...      ...        ...                 ...                ...                ...                ...                ...\n",
       "                              ...                  ...                  ...              ...               ...             ...              ...          ...          ...        ...       ...                 ...      ...      ...       ...                  ...         ...      ...        ...                 ...                ...                ...                ...                ...\n",
       "Dask Name: read-parquet, 1 graph layer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/uber-lyft-tlc/\",\n",
    ")\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has not loaded the data yet, it has:\n",
    "- investigated the input path and found that there are 720 files\n",
    "- intelligently created a set of jobs for each chunk -- one per original parquet file in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the representation of the `DataFrame` object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation\n",
    "\n",
    "Most Dask Collections, including Dask `DataFrame` are evaluated lazily, which means Dask constructs the logic (called task graph) of your computation immediately but \"evaluates\" them  only when necessary. You can view this task graph using `.visualize()`.\n",
    "\n",
    "We need to call `.compute()` to trigger actual computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions like `len` and `head` also trigger a computation. Specifically, calling `len` will:\n",
    "- load actual data, (that is, load each file into a pandas DataFrame)\n",
    "- then apply the corresponding functions to each pandas DataFrame (also known as a partition)\n",
    "- combine the subtotals to give you the final grand total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783431901"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and count number of rows\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the start and end of the data as you would in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02867</td>\n",
       "      <td>B02867</td>\n",
       "      <td>2019-02-01 00:01:26</td>\n",
       "      <td>2019-02-01 00:02:55</td>\n",
       "      <td>2019-02-01 00:05:18</td>\n",
       "      <td>2019-02-01 00:14:57</td>\n",
       "      <td>245</td>\n",
       "      <td>251</td>\n",
       "      <td>2.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.480000</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02879</td>\n",
       "      <td>B02879</td>\n",
       "      <td>2019-02-01 00:26:08</td>\n",
       "      <td>2019-02-01 00:41:29</td>\n",
       "      <td>2019-02-01 00:41:29</td>\n",
       "      <td>2019-02-01 00:49:39</td>\n",
       "      <td>216</td>\n",
       "      <td>197</td>\n",
       "      <td>1.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2019-02-01 00:48:58</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-02-01 00:51:34</td>\n",
       "      <td>2019-02-01 01:28:29</td>\n",
       "      <td>261</td>\n",
       "      <td>234</td>\n",
       "      <td>5.01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.970001</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2019-02-01 00:02:15</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-02-01 00:03:51</td>\n",
       "      <td>2019-02-01 00:07:16</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.390000</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2019-02-01 00:06:17</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-02-01 00:09:44</td>\n",
       "      <td>2019-02-01 00:39:56</td>\n",
       "      <td>87</td>\n",
       "      <td>198</td>\n",
       "      <td>6.84</td>\n",
       "      <td>...</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "0            HV0003               B02867               B02867   \n",
       "1            HV0003               B02879               B02879   \n",
       "2            HV0005               B02510                 <NA>   \n",
       "3            HV0005               B02510                 <NA>   \n",
       "4            HV0005               B02510                 <NA>   \n",
       "\n",
       "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0 2019-02-01 00:01:26 2019-02-01 00:02:55 2019-02-01 00:05:18   \n",
       "1 2019-02-01 00:26:08 2019-02-01 00:41:29 2019-02-01 00:41:29   \n",
       "2 2019-02-01 00:48:58                 NaT 2019-02-01 00:51:34   \n",
       "3 2019-02-01 00:02:15                 NaT 2019-02-01 00:03:51   \n",
       "4 2019-02-01 00:06:17                 NaT 2019-02-01 00:09:44   \n",
       "\n",
       "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  sales_tax  \\\n",
       "0 2019-02-01 00:14:57           245           251        2.45  ...       0.83   \n",
       "1 2019-02-01 00:49:39           216           197        1.71  ...       0.70   \n",
       "2 2019-02-01 01:28:29           261           234        5.01  ...       3.99   \n",
       "3 2019-02-01 00:07:16            87            87        0.34  ...       0.64   \n",
       "4 2019-02-01 00:39:56            87           198        6.84  ...       2.16   \n",
       "\n",
       "   congestion_surcharge  airport_fee  tips  driver_pay  shared_request_flag  \\\n",
       "0                   0.0          NaN   0.0    7.480000                    Y   \n",
       "1                   0.0          NaN   2.0    7.930000                    N   \n",
       "2                   0.0          NaN   0.0   35.970001                    N   \n",
       "3                   0.0          NaN   3.0    5.390000                    N   \n",
       "4                   0.0          NaN   4.0   17.070000                    N   \n",
       "\n",
       "   shared_match_flag  access_a_ride_flag  wav_request_flag wav_match_flag  \n",
       "0                  N                   N                 N            NaN  \n",
       "1                  N                   N                 N            NaN  \n",
       "2                  Y                   N                 N            NaN  \n",
       "3                  Y                   N                 N            NaN  \n",
       "4                  Y                   N                 N            NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computations with `dask.dataframe`\n",
    "\n",
    "Let's compute the maximum of the flight delay.\n",
    "\n",
    "With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "`dask.dataframe` lets us write pandas-like code, that operates on larger-than-memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 3.78 ms, total: 17 ms\n",
      "Wall time: 4.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4894.62"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "result = ddf.driver_pay.max()\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the lazy computation for us and then runs it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This means you can handle datasets that are larger than memory but, repeated computations will have to load all of the data in each time. (Run the code above again, is it faster or slower than you would expect?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the underlying task graph using `.visualize()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Intermediate Results\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` stores the arguments, allowing duplicate computations to be shared and only computed once.\n",
    "\n",
    "For example, let's compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren't the final results yet. They're just the steps required to get the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.persist()`\n",
    "\n",
    "While using a distributed scheduler, you can keep some _data that you want to use often_ in the _distributed memory_. \n",
    "\n",
    "`persist` generates \"Futures\" and stores them in the same structure as your output. You can use `persist` with any data or computation that fits in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num',\n",
       "       'request_datetime', 'on_scene_datetime', 'pickup_datetime',\n",
       "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
       "       'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
       "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
       "       'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
       "       'wav_request_flag', 'wav_match_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this section you will do a few `dask.dataframe` computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call `.compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: User persist to read the data only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many rows are in our dataset?\n",
    "\n",
    "_Hint_: how would you check how many items are in a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783431901"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In total, how many rides were tipped?\n",
    "\n",
    "_Hint_: use [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124533228"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddf[ddf.tips > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In total, how many rides were operated per operator?\n",
    "\n",
    "*Hint*: use [groupby](https://pandas.pydata.org/pandas-docs/stable/groupby.html). \"hvfhs_license_num\" is the operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num\n",
       "HV0002      6388934\n",
       "HV0003    561586224\n",
       "HV0004     13884957\n",
       "HV0005    201571786\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.groupby(\"hvfhs_license_num\").size().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What was the average driver pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.33131671006591"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.driver_pay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How big is the payout percentage to the drivers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8088178"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ddf.driver_pay.sum() / ddf.base_passenger_fare.sum()).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let's calculate the percentage of tipped rides per operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num\n",
       "HV0002    0.084400\n",
       "HV0003    0.149856\n",
       "HV0004    0.092949\n",
       "HV0005    0.191230\n",
       "Name: tipped, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf[\"tipped\"] = ddf.tips != 0\n",
    "ddf.groupby(\"hvfhs_license_num\").tipped.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data once and store our dataset in memory, this makes our computations very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom code with Dask DataFrame\n",
    "\n",
    "`dask.dataframe` only covers a small but well-used portion of the pandas API.\n",
    "\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel, e.g, sorting.\n",
    "\n",
    "Additionally, some important operations like `set_index` work, but are slower than in pandas because they include substantial shuffling of data, and may write out to disk.\n",
    "\n",
    "**What if you want to use some custom functions that aren't (or can't be) implemented for Dask DataFrame yet?**\n",
    "\n",
    "You can open an issue on the [Dask issue tracker](https://github.com/dask/dask/issues) to check how feasible the function could be to implement, and you can consider contributing this function to Dask.\n",
    "\n",
    "In case it's a custom function or tricky to implement, `dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
    "\n",
    "- [`map_partitions`](https://docs.dask.org/en/latest/generated/dask.dataframe.DataFrame.map_partitions.html): to run a function on each partition (each pandas DataFrame) of the Dask DataFrame\n",
    "- [`map_overlap`](https://docs.dask.org/en/latest/generated/dask.dataframe.rolling.map_overlap.html): to run a function on each partition (each pandas DataFrame) of the Dask DataFrame, with some rows shared between neighboring partitions\n",
    "- [`reduction`](https://docs.dask.org/en/latest/generated/dask.dataframe.Series.reduction.html): for custom row-wise reduction operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the `map_partitions()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method map_partitions in module dask.dataframe.core:\n",
      "\n",
      "map_partitions(func, *args, **kwargs) method of dask.dataframe.core.DataFrame instance\n",
      "    Apply Python function on each DataFrame partition.\n",
      "    \n",
      "    Note that the index and divisions are assumed to remain unchanged.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    func : function\n",
      "        The function applied to each partition. If this function accepts\n",
      "        the special ``partition_info`` keyword argument, it will receive\n",
      "        information on the partition's relative location within the\n",
      "        dataframe.\n",
      "    args, kwargs :\n",
      "        Positional and keyword arguments to pass to the function.\n",
      "        Positional arguments are computed on a per-partition basis, while\n",
      "        keyword arguments are shared across all partitions. The partition\n",
      "        itself will be the first positional argument, with all other\n",
      "        arguments passed *after*. Arguments can be ``Scalar``, ``Delayed``,\n",
      "        or regular Python objects. DataFrame-like args (both dask and\n",
      "        pandas) will be repartitioned to align (if necessary) before\n",
      "        applying the function; see ``align_dataframes`` to control this\n",
      "        behavior.\n",
      "    enforce_metadata : bool, default True\n",
      "        Whether to enforce at runtime that the structure of the DataFrame\n",
      "        produced by ``func`` actually matches the structure of ``meta``.\n",
      "        This will rename and reorder columns for each partition,\n",
      "        and will raise an error if this doesn't work,\n",
      "        but it won't raise if dtypes don't match.\n",
      "    transform_divisions : bool, default True\n",
      "        Whether to apply the function onto the divisions and apply those\n",
      "        transformed divisions to the output.\n",
      "    align_dataframes : bool, default True\n",
      "        Whether to repartition DataFrame- or Series-like args\n",
      "        (both dask and pandas) so their divisions align before applying\n",
      "        the function. This requires all inputs to have known divisions.\n",
      "        Single-partition inputs will be split into multiple partitions.\n",
      "    \n",
      "        If False, all inputs must have either the same number of partitions\n",
      "        or a single partition. Single-partition inputs will be broadcast to\n",
      "        every partition of multi-partition inputs.\n",
      "    meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      "        An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      "        and column names of the output. This metadata is necessary for\n",
      "        many algorithms in dask dataframe to work.  For ease of use, some\n",
      "        alternative inputs are also available. Instead of a ``DataFrame``,\n",
      "        a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      "        can be provided (note that the order of the names should match the\n",
      "        order of the columns). Instead of a series, a tuple of ``(name,\n",
      "        dtype)`` can be used. If not provided, dask will try to infer the\n",
      "        metadata. This may lead to unexpected results, so providing\n",
      "        ``meta`` is recommended. For more information, see\n",
      "        ``dask.dataframe.utils.make_meta``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Given a DataFrame, Series, or Index, such as:\n",
      "    \n",
      "    >>> import pandas as pd\n",
      "    >>> import dask.dataframe as dd\n",
      "    >>> df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
      "    ...                    'y': [1., 2., 3., 4., 5.]})\n",
      "    >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      "    \n",
      "    One can use ``map_partitions`` to apply a function on each partition.\n",
      "    Extra arguments and keywords can optionally be provided, and will be\n",
      "    passed to the function after the partition.\n",
      "    \n",
      "    Here we apply a function with arguments and keywords to a DataFrame,\n",
      "    resulting in a Series:\n",
      "    \n",
      "    >>> def myadd(df, a, b=1):\n",
      "    ...     return df.x + df.y + a + b\n",
      "    >>> res = ddf.map_partitions(myadd, 1, b=2)\n",
      "    >>> res.dtype\n",
      "    dtype('float64')\n",
      "    \n",
      "    Here we apply a function to a Series resulting in a Series:\n",
      "    \n",
      "    >>> res = ddf.x.map_partitions(lambda x: len(x)) # ddf.x is a Dask Series Structure\n",
      "    >>> res.dtype\n",
      "    dtype('int64')\n",
      "    \n",
      "    By default, dask tries to infer the output metadata by running your\n",
      "    provided function on some fake data. This works well in many cases, but\n",
      "    can sometimes be expensive, or even fail. To avoid this, you can\n",
      "    manually specify the output metadata with the ``meta`` keyword. This\n",
      "    can be specified in many forms, for more information see\n",
      "    ``dask.dataframe.utils.make_meta``.\n",
      "    \n",
      "    Here we specify the output is a Series with no name, and dtype\n",
      "    ``float64``:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(myadd, 1, b=2, meta=(None, 'f8'))\n",
      "    \n",
      "    Here we map a function that takes in a DataFrame, and returns a\n",
      "    DataFrame with a new column:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y))\n",
      "    >>> res.dtypes\n",
      "    x      int64\n",
      "    y    float64\n",
      "    z    float64\n",
      "    dtype: object\n",
      "    \n",
      "    As before, the output metadata can also be specified manually. This\n",
      "    time we pass in a ``dict``, as the output is a DataFrame:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y),\n",
      "    ...                          meta={'x': 'i8', 'y': 'f8', 'z': 'f8'})\n",
      "    \n",
      "    In the case where the metadata doesn't change, you can also pass in\n",
      "    the object itself directly:\n",
      "    \n",
      "    >>> res = ddf.map_partitions(lambda df: df.head(), meta=ddf)\n",
      "    \n",
      "    Also note that the index and divisions are assumed to remain unchanged.\n",
      "    If the function you're mapping changes the index/divisions, you'll need\n",
      "    to clear them afterwards:\n",
      "    \n",
      "    >>> ddf.map_partitions(func).clear_divisions()  # doctest: +SKIP\n",
      "    \n",
      "    Your map function gets information about where it is in the dataframe by\n",
      "    accepting a special ``partition_info`` keyword argument.\n",
      "    \n",
      "    >>> def func(partition, partition_info=None):\n",
      "    ...     pass\n",
      "    \n",
      "    This will receive the following information:\n",
      "    \n",
      "    >>> partition_info  # doctest: +SKIP\n",
      "    {'number': 1, 'division': 3}\n",
      "    \n",
      "    For each argument and keyword arguments that are dask dataframes you will\n",
      "    receive the number (n) which represents the nth partition of the dataframe\n",
      "    and the division (the first index value in the partition). If divisions\n",
      "    are not known (for instance if the index is not sorted) then you will get\n",
      "    None as the division.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ddf.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Distance\" column in `ddf` is currently in miles. Let's say we want to convert the units to kilometers and we have a general helper function as shown below. In this case, we can use `map_partitions` to apply this function across each of the internal pandas `DataFrame`s in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7676685300.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.driver_pay.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_converter(df, multiplier=1):\n",
    "    df[\"driver_pay\"] = df[\"driver_pay\"] * multiplier\n",
    "    return df\n",
    "\n",
    "\n",
    "ddf = ddf.map_partitions(\n",
    "    my_custom_converter, multiplier=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4606011000.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.driver_pay.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `meta`?\n",
    "\n",
    "Since Dask operates lazily, it doesn't always have enough information to infer the output structure (which includes datatypes) of certain operations.\n",
    "\n",
    "`meta` is a _suggestion_ to Dask about the output of your computation. Importantly, `meta` _never infers with the output structure_. Dask uses this `meta` until it can determine the actual output structure.\n",
    "\n",
    "Even though there are many ways to define `meta`, we suggest using a small pandas Series or DataFrame that matches the structure of your final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Close you Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to always close any Dask cluster you create:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
